# ApolloX V_1.0 (Automatic Prediction by generative mOdel for Large-scaLe Optimization of X-composition materials)

## Overview
ApolloX (Automatic Prediction by generative mOdel for Large-scaLe Optimization of X-composition materials), a physics-guided computational framework designed for the structural prediction and discovery of AHEMs. ApolloX combines a conditional generative deep learning model with particle swarm optimization (PSO), leverag-ing chemical short-range order (CSRO) descriptors encoded as Pair Density Matrices (PDMs). By correlating PDM constraints with enthalpic thermostability, ApolloX effectively narrows the structural search space and bridges the gap between local atomic arrangements and the global energy landscape. Through guided structural generation and optimization, ApolloX successfully manages the disorder inherent in amorphous systems while providing a scalable, energy-driven methodology for multi-component materials.

## Features
- **Generated base on short-range order**: Short-range order alone is sufficient to generate amorphous multi-component structures.
- **Cond-CDVAE Structure Generation**: Employs a conditional generative model to predict feasible crystal structures based on input chemical compositions and desired properties.
- **Material Types**: Supports generation of nano, bulk, and perovskite high-entropy alloys.
- **Maximum atomic number scale**: Accommodate models of high-entropy materials with occupancies at the thousandth percentile.

## System Requirements

- OS: Linux (Ubuntu 20.04+),
- Python Version: 3.9 or later
- GPU: Optional but recommended for deep learning models (e.g., NVIDIA H20, NVIDIA A800 or better)
- CUDA Version: 11.8 (required for GPU acceleration with PyTorch)
- Tested on:
  - Ubuntu 22.04 + CUDA 11.8

Dependencies include:
- torch==2.0.1
- torch_geometric (pyg_lib, torch_scatter, etc.)
- numpy, pandas, scikit-learn


# 1. Installation
> Typical install time: 5–10 minutes on a standard desktop with internet access.
~~~~bash
#Create a new environment. "myenv" is the name of the environment.
conda create -n myenv python=3.10 -y
conda init bash
source ~/.bashrc

#Activate the environment.
conda activate myenv

# Clone the repository
git clone https://github.com/FNC001/ApolloX.git
cd ApolloX/

# Create a new environment
conda create -n myenv python=3.10 -y

# Install required libraries
cd cond-cdvae
pip install -r requirements.txt
pip install -e .
~~~~

---

# 2. Environment Variables

Enter ApolloX/cond-cdvae:
~~~~bash
chmod +x gen_env.sh
~~~~
~~~~bash
./gen_env.sh
~~~~
Then the environment variables can be written automatically, such as:

export PROJECT_ROOT=/path/to/this/project

export HYDRA_JOBS=/path/to/this/project/log

You can check them by `vi .env`.

---

# 3. Data Preparation

## 3.1 Prepare the original structure
Put a POSCAR file in "~/ApolloX/original_structures". The dataset will be generated by shuffling the coordinates of the structure in this file. The name of the POSCAR file should start with "POSCAR".

## 3.2 Generate the dataset
~~~~bash
cd ~/ApolloX/prepare_dataset
~~~~

~~~~bash
vi config.yaml
~~~~

In ```~/ApolloX/prepare_dataset/config.yaml```, you can set the dataset parameters:

~~~~bash
#initial_structure:the file name of the original structure in "~/ApolloX/original_structures"
initial_structure: POSCAR

#num:the size of the dataset
num: 100

#generation_type: (single or variable)
#If you choose "single", structures with the same stoichiometry will be generated.
#If you choose "variable", structures with fixed total number of atoms but varying stoichiometries will be generated.
generation_type: single

#dataset_path: Path to the generated structures
dataset_path: ~/autodl-tmp/prepare_data

#cutoff: cutoff radius used for computing the pair distribution matrix
cutoff: 5

#n_jobs: number of cores used for parallel processing
n_jobs: 4

#mode: (pair triple quadruple) type of the pair distribution matrix
#By setting "pair", "triple", or "quadruple", only the corresponding two-body, three-body, or four-body matrix elements will be considered.
#By setting "pair triple", both two-body and three-body matrix elements will be considered.
#By setting "pair triple quadruple", all the types of matrix elements will be considered. 
mode: pair

#the ratio of the training set, test set, and validation set, with their sum equal to 1.
train_ratio: 0.8
test_ratio: 0.1
val_ratio: 0.1
~~~~

Run ```generate_dataset_main.py```
~~~~python
python generate_dataset_main.py
~~~~


The mean and standard deviation of the training set will be saved in the ```dataset_path```(in this case, ```~/autodl-tmp/prepare_data```) directory under the filename ```scaler_stats.txt```. These statistics can be used to standardize other pair distribution matrices. 


---

# 4. Training the Cond-CDVAE Model

1. Go to the config folder, for example:

   ~~~~bash
   cd ~/ApolloX/cond-cdvae/conf/data/apollox
   ~~~~

2. Edit the `.yaml` file (e.g., `mp_apollox.yaml`) to set `root_path` correctly:

   ~~~~yaml
   root_path: ~/autodl-tmp/prepare_data  # Keep the same as the parameter "dataset_path" in "~/ApolloX/prepare_dataset/config.yaml"
   prop:
     - pressure
   num_targets:
     - 1
   niggli: true
   primitive: False
   graph_method: crystalnn
   lattice_scale_method: scale_length
   preprocess_workers: 30
   readout: mean
   max_atoms: 200
   otf_graph: false
   eval_model_name: calypso
   conditions:
     - composition
   ~~~~

3. Start training, for example:

   ~~~~bash
   CUDA_VISIBLE_DEVICES=0 HYDRA_FULL_ERROR=1 nohup python -u ~/ApolloX/cond-cdvae/cdvae/run.py  \
   model=vae data=apollox/mp_apollox project=apollox group=Group_name expname=Exp_name \
   optim.optimizer.lr=1e-4 optim.lr_scheduler.min_lr=1e-5 model.zgivenc.no_mlp=False \
   model.predict_property=False model.encoder.hidden_channels=128 model.encoder.int_emb_size=128 \
   model.encoder.out_emb_channels=128 model.latent_dim=128 model.encoder.num_blocks=4 \
   model.decoder.num_blocks=4 model.conditions.types.pressure.n_basis=80 model.conditions.types.pressure.stop=5 \
   train.pl_trainer.devices=1 +train.pl_trainer.strategy=ddp_find_unused_parameters_true \
   model.prec=32 data.teacher_forcing_max_epoch=60 logging.wandb.mode=offline model.cost_lattice=1 \
   > ./apollox_yourname.log 2>&1 &
   ~~~~

   ```CUDA_VISIBLE_DEVICES``` and ```train.pl_trainer.devices``` can be changed according to the number of GPU. For example, if you use two GPUs to train, ```CUDA_VISIBLE_DEVICES=0,1``` and ```train.pl_trainer.devices=2```. The training log is saved in ```apollox_yourname.log```. The trained model can be used in ```~/ApolloX/cond-cdvae/log/singlerun/apollox/Group_name/Exp_name```.

---

# 5. Generate new structures
   Enter the path to the trained model:
   ~~~~bash
   cd ~/ApolloX/cond-cdvae/log/singlerun/apollox/Group_name/Exp_name
   ~~~~
   
   Generate new structures:
   ~~~~bash
   CUDA_VISIBLE_DEVICES=0 python ~/ApolloX/cond-cdvae/scripts/evaluate.py \
       --model_path `pwd` \
       --tasks gen \
       --formula=B12Co12Fe12Mo12Ni12O60 \
       --pressure=0 \
       --label=B12Co12Fe12Mo12Ni12O60 \
       --element_values="2.33, 4.94, 5.78, 4.67, 5.00, 8.22, ..." \
       --batch_size=1\
       --num_batches_to_samples=20
   ~~~~

   - `--element_values` should be the **standardized** PDM (e.g., from `~/autodl-tmp/prepare_data/train_set_scaled.csv`).  
   - If you have new PDM data, you can use `scaler_stats.txt` to standardize it:
   ~~~~bash
   python ~/ApolloX/prepare_dataset/standardize.py --input path/to/original_PDM.csv --scaler ~/autodl-tmp/prepare_data/scaler_stats.txt --output ~/autodl-tmp/prepare_data/standardized_data.csv
   ~~~~
   - num_batches_to_samples: number of samples from batches. The total number of generated structures is "batch_size × num_batches_to_samples"

   The parameters:
   
   - input: path to original PDM
   
   - scaler: path to "scaler_stats.txt"(see the parameter ```dataset_path``` in ```~/ApolloX/prepare_dataset/config.yaml```)
   
   - output: path to the standardized data
   
   You can find the standardized PDM in the ```~/autodl-tmp/prepare_data/standardized_data.csv```.

   An example of ```original_PDM.csv```:
   ~~~~bash
  material_id,cif_file,BB,BCo,BFe,BMo,BNi,BO,CoCo,CoFe,CoMo,CoNi,CoO,FeFe,FeMo,FeNi,FeO,MoMo,MoNi,MoO,NiNi,NiO,OO
POSCAR,POSCAR.cif,50,96,180,98,166,142,42,153,89,157,111,125,146,253,214,52,153,146,117,187,83
   ~~~~

   ```standardized_data.csv```:
   ~~~~bash
   material_id,cif_file,BB,BCo,BFe,BMo,BNi,BO,CoCo,CoFe,CoMo,CoNi,CoO,FeFe,FeMo,FeNi,FeO,MoMo,MoNi,MoO,NiNi,NiO,OO
POSCAR,POSCAR.cif,-0.5378625865551956,-0.8989024944151758,-0.45835836723035484,-0.144069870640791,-0.8677595315897402,1.3962451958620272,-0.617216706313756,-0.8260540265833325,-0.8413829632040266,-0.8862085589245776,-0.54078126222574,-0.4847583483377272,-0.21715020776053026,-0.8796575998230133,1.9058206674509943,-0.06469889994525867,-0.8708426374852465,2.86515022376605,-0.6608800211032471,-0.7144005327292688,3.9482970076032293
   ~~~~

  After generating structures, a file named ```eval_gen_×××.pt``` is created. 
  
  Run
  ~~~~python
  python ~/ApolloX/cond-cdvae/scripts/extract_gen.py eval_gen_×××.pt
  ~~~~
  Structures can be found in ```eval_gen_×××/gen```.
  
---

# 6. Batch Generation and Optimization (PSO)

This algorithm includes:
- Generate initial structures
- Use the Generative Model to create new structures
- Optimize the generated structures

  ![picture](https://private-user-images.githubusercontent.com/181531316/473274724-9fd85981-b6bf-463a-8ada-af45440ad5c8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQwMjAzMTgsIm5iZiI6MTc1NDAyMDAxOCwicGF0aCI6Ii8xODE1MzEzMTYvNDczMjc0NzI0LTlmZDg1OTgxLWI2YmYtNDYzYS04YWRhLWFmNDU0NDBhZDVjOC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODAxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgwMVQwMzQ2NThaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02Y2E4N2I3NDY0MmM4MjE1ZWVhZmM1ZDEzM2YxNDc2MzZlMjM1Yjg3MDE4NWJlZDYxZmU0NmI5YzBiY2EzZTAyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.1fYsbAiOE1RkPGR-fbLfOabIl3-8UBNOJF_zzIZCz-w)

Enter your trained model path, for example:
~~~~bash
cd ~/ApolloX/cond-cdvae/log/singlerun/apollox/Group_name/Exp_name
~~~~

Set parameters:

~~~~bash
vi ~/ApolloX/PSO/config.yaml
~~~~

Parameters:
~~~~bash
poscar_name: POSCAR        #name of the original structure's POSCAR file (in "~/ApolloX/original_structures")
gen_num: 15                #the number of generation
structure_num_per_gen: 100 #the number of generated structures in each generation
opt_script: chgnet_cpu.py  #set "chgnet_gpu.py" if you use GPU to optimize the structures
output_dir: poscar         #the folder of generated structures
scaler_path: path/to/scaler_stats.txt #see the parameter "dataset_path" in "~/ApolloX/prepare_dataset/config.yaml"
# opt parameters
opt:
  mlp_optstep: 1        #Number of optimization steps per call using the machine learning potential
  fmax: 0.02            #Force convergence threshold in eV/Å; optimization stops when all atomic forces are below this value
  max_workers: 5        #Maximum number of parallel workers (threads or processes) used during optimization
  min_free_mem_gb: 4.0  #Minimum required free memory (in GB) to start a job

# PDM parameters (keep the same as "~/ApolloX/prepare_dataset/config.yaml" )
pdm:
  cutoff: 5 
  n_jobs: 4
  mode: pair
  output_csv: all_structures_summary.csv

#PSO parameters
PSO:
  target_ratio: 0.6    #The target fraction of structures to retain after each generation (e.g., top 60% are kept)
  min_bound_scale: 0.8 #Lower bound scaling factor for atomic displacement or search space (e.g., 80% of original scale)
  max_bound_scale: 1.2 #Upper bound scaling factor for atomic displacement or search space (e.g., 120% of original scale)
  max_iter: 100        #Maximum number of PSO iterations (generations) to perform
~~~~

Run ```~/ApolloX/PSO/run_generations_main.py```:

~~~~python
python ~/ApolloX/PSO/run_generations_main.py
~~~~

Then, a folder ```poscar``` is created.  ```Generation 1``` has 100 initial optimized structures. ```Generation 2```, ```Generation3```,···```Generation16``` have 60 optimized structures by "cond-cdvae+PSO" and 40 random structures (named "POSCAR-shuffled-×××.vasp") separately.

---


## Contributing
Contributions to ApolloX are welcome. Please submit your pull requests to the repository.

## License

This project is licensed under the MIT License.  
You are free to use, modify, and distribute this software with proper attribution.  

MIT License

Copyright (c) 2024 Honglin Li & Yongfeng Guo & Xiaoshan Luo, Wanlu Li & Yanchao Wang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


## Cite link
https://doi.org/10.48550/arXiv.2503.07043

## Reproducibility

To facilitate reproducibility of our results, we provide all necessary resources on the url "https://doi.org/10.5281/zenodo.15285136". This includes:

- The **training dataset** used in the manuscript (`data/train_set_scaled.csv`, etc.)
- The **trained Cond-CDVAE model** checkpoints used for structure generation and evaluation (`models/cond_cdvae_model.pt` or similar)
- A configuration YAML file (`apollox.yaml`) matching the setup used in the paper

## Acknowledgments

